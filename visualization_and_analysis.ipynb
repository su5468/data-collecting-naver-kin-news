{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기본 시각화 및 전처리\n",
    "데이터 가져와서 결측값 삭제하고 자료형 변환  \n",
    "이후 연도별 Frequency를 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 코드는 윈도우 기준임.\n",
    "# 타 OS에서는 정상작동을 보장하지 않으니 확인 요함.\n",
    "\n",
    "import datetime as dt\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dateutil.parser as dtparser\n",
    "\n",
    "import utils\n",
    "from remove_similar_articles import jaccard\n",
    "\n",
    "plt.rc('font', family=\"Malgun Gothic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 파일들의 파일명과 검색 키워드를 딕셔너리에 저장.\n",
    "# 순회해서 데이터를 쉽게 불러오기 위한 작업임.\n",
    "\n",
    "# 파일 종류를 저장하는 문자열 리스트\n",
    "source_types = [\"news_api\", \"news_crawl\", \"kin\", \"news_unique\", \"kin_unique\"]\n",
    "\n",
    "# keywords[\"파일종류 문자열\"]: 해당 파일종류가 가지고 있는 키워드들의 리스트.\n",
    "# 병합, 중복제거, 토큰화 결과(news_unique)는 키워드가 없으므로 빈 문자열을 키워드로 함.\n",
    "keywords = {}\n",
    "keywords[\"kin\"] = [\"환자 권리\", \"환자 요구\", \"환자 의견\"]\n",
    "keywords[\"news\"] = []\n",
    "keywords[\"news_api\"] = [\"환자-의사 공유 의사결정\"]\n",
    "keywords[\"news_crawl\"] = [\"환자 의사 공유의사결정\"]\n",
    "keywords[\"news_unique\"] = [\"\"]\n",
    "keywords[\"kin_unique\"] = [\"\"]\n",
    "\n",
    "# filenames[\"파일종류 문자열\"]: 해당 파일종류의 실제 파일명.\n",
    "filenames = {}\n",
    "filenames[\"kin\"] = utils.FileType.KIN_WT.value\n",
    "filenames[\"news\"] = \"\"\n",
    "filenames[\"news_api\"] = utils.FileType.NEWS_WT.value\n",
    "filenames[\"news_crawl\"] = utils.FileType.CRAWL_NEWS_WT.value\n",
    "filenames[\"news_unique\"] = utils.FileType.NEWS_PROCESSED_UNIQUE.value\n",
    "filenames[\"kin_unique\"] = utils.FileType.KIN_PROCESSED_UNIQUE.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제로 데이터프레임을 로드함.\n",
    "# dfs[\"파일종류 문자열\"][\"키워드 문자열\"]: 파일종류와 키워드가 지정된 데이터프레임.\n",
    "# e.g. dfs[\"news_crawl\"][\"환자 의사 공유의사결정\"]은 \"환자 의사 공유의사결정\" 검색어로 검색해서 크롤링으로 수집한 뉴스 데이터프레임.\n",
    "\n",
    "dfs = {}\n",
    "for source_type in source_types:\n",
    "    dfs[source_type] = {}\n",
    "    for keyword in keywords[source_type]:\n",
    "        filename = f'{filenames[source_type]}_{keyword}.txt' if keyword else f'{filenames[source_type]}.txt'\n",
    "        data_json = utils.get_json_from_file(filename)[\"items\"]\n",
    "        dfs[source_type][keyword] = pd.DataFrame(data_json)\n",
    "\n",
    "\n",
    "# 데이터프레임 가공 및 전처리.\n",
    "\n",
    "# 지식인 데이터 전처리를 위한 함수\n",
    "def dtparse(s: str) -> None | dt.datetime :\n",
    "    \"\"\"\n",
    "    지식인 데이터에서 문자열을 dt객체로 파싱하는 함수.\n",
    "    파싱에 실패하면 None 반환.\n",
    "\n",
    "    Args:\n",
    "        s (str): 날짜를 나타낼 것으로 추측되는 문자열.\n",
    "\n",
    "    Returns:\n",
    "        None | dt.datetime: 파싱이 성공하면 dt객체, 실패하면 None.\n",
    "    \"\"\"\n",
    "    if s.startswith(\"작성일\"):\n",
    "        s = s[3:]\n",
    "    try:\n",
    "        return dtparser.parse(s)\n",
    "    except dtparser.ParserError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# 네이버 뉴스 api 데이터.\n",
    "# 날짜를 dt객체로 변경.\n",
    "# 에러로 인해 수집되지 않은 데이터 드롭.\n",
    "source_type = \"news_api\"\n",
    "for keyword in keywords[source_type]:\n",
    "    df = dfs[source_type][keyword]\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df[~df[\"text\"].str.endswith(\"_error\")]\n",
    "    dfs[source_type][keyword] = df\n",
    "\n",
    "\n",
    "# 네이버 뉴스 크롤링 데이터.\n",
    "# 날짜를 dt객체로 변경.\n",
    "# 에러로 인해 수집되지 않은 데이터 드롭.\n",
    "source_type = \"news_crawl\"\n",
    "for keyword in keywords[source_type]:\n",
    "    df = dfs[source_type][keyword]\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df[~df[\"text\"].str.endswith(\"_error\")]\n",
    "    dfs[source_type][keyword] = df\n",
    "\n",
    "\n",
    "# 네이버 지식인 데이터.\n",
    "# 날짜를 나타내는 \"지저분한\" 문자열을 dt객체로 변경.\n",
    "source_type = \"kin\"\n",
    "for keyword in keywords[source_type]:\n",
    "    df = dfs[source_type][keyword]\n",
    "    df[\"date\"] = df[\"date\"].apply(lambda x: list(map(dtparse, x)))\n",
    "    dfs[source_type][keyword] = df\n",
    "\n",
    "\n",
    "# 중복을 제거하고 토큰화한 뉴스 데이터.\n",
    "# api와 크롤링에서 날짜 형식이 달라서 통일하고 dt객체로 반환.\n",
    "# 토큰이 없는 데이터 드롭.\n",
    "# TODO: 별로 깔끔한 방식은 아님. 리팩토링 요함.\n",
    "# 1. 수집 때부터 날짜 양식을 맞추도록 수정할 것.\n",
    "# 2. .ipynb 파일을 분리해서 전처리부와 시각화부를 나눌 것.\n",
    "source_type = \"news_unique\"\n",
    "for keyword in keywords[source_type]:\n",
    "    df = dfs[source_type][keyword]\n",
    "    df[\"date\"] = df[\"date\"].apply(lambda x: pd.to_datetime(x).tz_localize(None))\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df[df[\"tokens\"].apply(lambda x: len(x) > 0)]\n",
    "    dfs[source_type][keyword] = df\n",
    "\n",
    "\n",
    "# 중복을 제거하고 토큰화한 지식인 데이터.\n",
    "# 데이터프레임에서 질문, 답변을 분리하여 정규화.\n",
    "# 기존 인덱스를 id로 삼아서 질문과 답변의 연결을 유지\n",
    "# 이후 질문 데이터프레임과 답변 데이터프레임을 연결(concat).\n",
    "# 날짜를 나타내는 \"지저분한\" 문자열을 dt객체로 변경.\n",
    "source_type = \"kin_unique\"\n",
    "for keyword in keywords[source_type]:\n",
    "    df = dfs[source_type][keyword].copy()\n",
    "    df = df[df[\"date\"].apply(lambda x: len(x) > 0)]\n",
    "    df[\"id\"] = df.index\n",
    "\n",
    "    df_question = df.copy()\n",
    "    df_question[\"date\"] = df[\"date\"].apply(lambda x: x.pop(0))\n",
    "    df_question.drop(columns=[\"tokens_answer\", \"answers\"], inplace=True)\n",
    "    df_question.rename({\"question\": \"text\"}, axis=\"columns\", inplace=True)\n",
    "    df_question[\"type\"] = \"question\"\n",
    "\n",
    "    df_answer = df.explode(column=[\"answers\", \"date\", \"tokens_answer\"], ignore_index=True)\n",
    "    df_answer.drop(columns=[\"tokens\", \"question\"], inplace=True)\n",
    "    df_answer.rename({\"tokens_answer\": \"tokens\", \"answers\": \"text\"}, axis=\"columns\", inplace=True)\n",
    "    df_answer[\"type\"] = \"answer\"\n",
    "\n",
    "    df = pd.concat([df_question, df_answer], ignore_index=True)\n",
    "    df[\"date\"] = df[\"date\"].apply(dtparse)\n",
    "    dfs[source_type][keyword] = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_bound = 0.5\n",
    "fname = \"kin_answer_sim_cache.txt\"\n",
    "force_redo = input(\"지식IN 데이터의 답변 유사도를 재검사하겠습니까? (Y/N)\")\n",
    "if force_redo.lower() != \"y\" and utils.already(fname):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        similarity = pickle.load(f)\n",
    "    to_del = [i for i, e in enumerate(similarity) if e > sim_bound] # TODO\n",
    "else:\n",
    "    n = len(df_answer)\n",
    "    similarity = [[0] * n for _ in range(n)]\n",
    "    to_del = []\n",
    "    delition = [False] * n\n",
    "    for i, row1 in df_answer.iterrows():\n",
    "        if not i % 100:\n",
    "            print(f\"{i}'th complete\")\n",
    "        if delition[i]:\n",
    "            continue\n",
    "        a = np.array(row1[\"tokens\"])\n",
    "        for j in range(i + 1, n):\n",
    "            if delition[j]:\n",
    "                continue\n",
    "            b = np.array(df_answer.loc[j, \"tokens\"])\n",
    "            similarity[i][j] = similarity[j][i] = jaccard(a, b)\n",
    "            if similarity[i][j] > sim_bound:\n",
    "                delition[j] = True\n",
    "                to_del.append(j)\n",
    "    with open(fname, \"wb\") as f:\n",
    "        pickle.dump(similarity, f)\n",
    "\n",
    "df_answer.drop(index=to_del, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year Frequency 시각화\n",
    "# TODO: 시각화와 전처리를 다른 파일로 분리하고, 각 파일타입별로 셀을 분리.\n",
    "\n",
    "# 네이버 뉴스 api 데이터.\n",
    "# 연도를 뽑아내고 연도별 countplot 플로팅.\n",
    "source_type = \"news_api\"\n",
    "for keyword in keywords[source_type]:\n",
    "    df = dfs[source_type][keyword]\n",
    "    df[\"year\"] = df[\"date\"].dt.year\n",
    "    sns.countplot(data=df, x=\"year\", color=\"green\")\n",
    "    plt.title(f\"Year Frequency of News Data - Naver API (N={len(df)})\")\n",
    "    plt.show()\n",
    "\n",
    "# 네이버 뉴스 크롤링 데이터.\n",
    "# 연도를 뽑아내고 연도별 countplot 플로팅.\n",
    "source_type = \"news_crawl\"\n",
    "for keyword in keywords[source_type]:\n",
    "    df = dfs[source_type][keyword]\n",
    "    df[\"year\"] = df[\"date\"].dt.year\n",
    "    sns.countplot(data=df, x=\"year\", color=\"green\")\n",
    "    plt.title(f\"Year Frequency of News Data (2000 - 2023, N={len(df)})\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# 네이버 지식인 데이터.\n",
    "# 전체 n을 세고, 년도를 기록함.\n",
    "# 이후 해당 데이터를 이용해 barplot 플로팅.\n",
    "# TODO: 리팩토링 요함.\n",
    "# 1. 전처리 과정 등에서 데이터 concat 필요.\n",
    "# 2. 각 파일에 별도로 접근해야 하는 경우를 대비해 keyword 열을 생성.\n",
    "source_type = \"kin\"\n",
    "temp_n = 0\n",
    "year_dict = dict(zip(keywords[source_type], [defaultdict(int) for _ in keywords[source_type]]))\n",
    "for keyword in keywords[source_type]:\n",
    "    df = dfs[source_type][keyword]\n",
    "    temp_n += len(df)\n",
    "    for row in df[\"date\"]:\n",
    "        for date in row:\n",
    "            if date is None:\n",
    "                continue\n",
    "            year_dict[keyword][date.year] += 1\n",
    "\n",
    "df_temp = pd.melt(pd.DataFrame(year_dict).reset_index(), id_vars='index', var_name='column', value_name='value')\n",
    "sns.barplot(data=df_temp, x='index', y='value', hue='column')\n",
    "plt.title(f\"Year Frequency of Naver KIN Data (N={temp_n})\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title = \"keyword\")\n",
    "plt.show()\n",
    "\n",
    "# 중복을 제거하고 토큰화한 뉴스 데이터.\n",
    "# 연도를 뽑아내고 연도별 countplot 플로팅.\n",
    "source_type = \"news_unique\"\n",
    "for keyword in keywords[source_type]:\n",
    "    df = dfs[source_type][keyword]\n",
    "    df[\"year\"] = df[\"date\"].dt.year\n",
    "    sns.countplot(data=df, x=\"year\", color=\"green\")\n",
    "    plt.title(f\"Year Frequency of Whole News Data - Deduplicated (N={len(df)})\")\n",
    "    plt.xticks(rotation=66)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 키워드 별 기사 빈도 플로팅.\n",
    "source_type = \"news_unique\"\n",
    "for keyword in keywords[source_type]:\n",
    "    df = dfs[source_type][keyword]\n",
    "    df[\"year\"] = df[\"date\"].dt.year\n",
    "    df = df[[\"year\", \"keyword\"]]\n",
    "    df_count = df.value_counts().unstack().fillna(0)\n",
    "\n",
    "    df_count.plot.bar(stacked=True, legend=\"reverse\", width=0.8)\n",
    "    plt.title(f\"Year Frequency of Whole News Data - Deduplicated (N={len(df)})\")\n",
    "    plt.xticks(rotation=66)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토픽 모델링\n",
    "BERTopic으로 Hierarchical Topic Modeling 수행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab 모듈의 경우 python-mecab-ko 모듈을 사용함.\n",
    "# 이 모듈이 느리기 때문에, 성능개선을 위해서는 다른 토크나이저를 사용하도록 할 수 있음.\n",
    "\n",
    "import re\n",
    "from itertools import chain\n",
    "from typing import List\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from mecab import MeCab\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 뉴스 토픽모델링으로 유관 데이터 분석.\n",
    "# TODO: 전처리, 시각화, 토픽모델링을 각 파일로 분리.\n",
    "typestring = input('대상 데이터 종류 선택(\"news\" | \"kin\")')\n",
    "serial_version = input('불러올 모델 선택(\"v1\", \"v2\", ..., 빈칸이면 새로 학습)')\n",
    "\n",
    "\n",
    "df = dfs[f\"{typestring}_unique\"][\"\"]\n",
    "n = len(df)\n",
    "mecab = MeCab()\n",
    "\n",
    "# 사용중인 토크나이저의 성능 이슈 때문에, 문서 별로 이미 캐시된 토큰화 결과를 그대로 사용함.\n",
    "# betopic에서 토큰화를 수행할 때, 문서들을 concat해서 수행하는데, 문서 단위의 토큰화 결과는 동일하므로, 이를 재사용하는 게 목적.\n",
    "# deli: concat한 문서를 다시 분리하기 위한 구분자(delimeter)임. 원본 문서들에 없는 문자열이면 무엇이든 사용 가능.\n",
    "# token_map[\"문서내용\"]: 해당 문서내용의 토큰화 결과\n",
    "deli = \"$!$!$!\"\n",
    "token_map = dict(zip(\n",
    "    df[\"text\"].apply(lambda x: re.sub(r\"\\s\", \"\", x).lower()), \n",
    "    df[\"tokens\"]\n",
    "))\n",
    "token_map[\"\"] = []\n",
    "\n",
    "# 불용어를 일일히 설정하는 대신 길이가 1인 토큰은 전부 드랍.\n",
    "# 추가로 필요하면 vectorizer에 불용어 추가 가능하니 추가하면 됨.\n",
    "for key, val in token_map.items():\n",
    "    token_map[key] = list(filter(lambda x: len(x) >= 2, val))\n",
    "\n",
    "def tokenize_nouns(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    캐시된 문서의 토큰 목록을 반환.\n",
    "\n",
    "    Args:\n",
    "        text (str): 문서 내용 전체.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 해당 문서의 토큰화 결과 리스트.\n",
    "    \"\"\"\n",
    "    texts = text.split(deli)\n",
    "    rets = list(chain.from_iterable([token_map[re.sub(\"(\\s|\\\\xa0)\", \"\", text)] for text in texts]))\n",
    "    return rets\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_nouns)\n",
    "\n",
    "# 추후 연결된 문서를 분리하기 위해 문서의 끝에 구분자를 삽입.\n",
    "docs = df[\"text\"].copy()\n",
    "for i in docs.index:\n",
    "    docs[i] += deli\n",
    "\n",
    "if serial_version:\n",
    "    # 직렬화한 토픽모델링 결과 로드.\n",
    "    topic_model = BERTopic.load(f\"{utils.MATERIALS}/bertopic_filter_{typestring}_{serial_version}\")\n",
    "    topic_model.vectorizer_model = vectorizer\n",
    "    topic_model.language = \"multilingual\"\n",
    "else:\n",
    "    # 토픽모델링 수행.\n",
    "    topic_model = BERTopic(verbose=True, language=\"multilingual\", vectorizer_model=vectorizer)\n",
    "    topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 직렬화하여 저장.\n",
    "\n",
    "v = 1\n",
    "while True:\n",
    "    if not utils.already(f\"{utils.MATERIALS}/bertopic_filter_{typestring}_v{v}\"):\n",
    "        break\n",
    "    v += 1\n",
    "\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "topic_model.save(f\"{utils.MATERIALS}/bertopic_filter_{typestring}_v{v}\", serialization=\"pytorch\", save_ctfidf=True, save_embedding_model=embedding_model)\n",
    "\n",
    "\n",
    "# 수동으로 연관된 것으로 판단되는 토픽 번호를 뽑아서 데이터 저장.\n",
    "# Cortext에 사용을 용이하게 하기 위해 토큰에 구분자 \"***\" 삽입.\n",
    "\n",
    "related_topic_nums = map(int, input('연관된 토픽 번호 입력(구분자: 띄어쓰기)').split())\n",
    "t = topic_model.get_document_info(docs)\n",
    "df_filtered = t[t[\"Topic\"].isin(related_topic_nums)]\n",
    "df_filtered[\"Tokens\"] = df_filtered[\"Document\"].apply(lambda x: '***'.join(tokenize_nouns(x.lower())))\n",
    "df_filtered.to_csv(f\"{utils.RESULTS}/naver_news_filtered_df_v3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토픽 정보를 확인하거나 계층적 토픽에 대한 덴드로그램을 그림.\n",
    "\n",
    "# 정보 확인, 시각화.\n",
    "# topic_model.get_topic_info()\n",
    "# topic_model.visualize_topics()\n",
    "\n",
    "# 덴드로그램.\n",
    "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
